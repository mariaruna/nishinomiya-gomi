import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import datetime
import re
from urllib.parse import urljoin

# --- è¨­å®š ---
st.set_page_config(page_title="è¥¿å®®å¸‚ã”ã¿ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼", page_icon="ğŸ—‘ï¸")

# ==========================================
# 1. ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼å–å¾—ãƒ»å‡¦ç†æ©Ÿèƒ½
# ==========================================
def get_url_by_date(year, month):
    """æŒ‡å®šã—ãŸå¹´æœˆã®å…¬å¼ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼URLã‚’ç”Ÿæˆã™ã‚‹"""
    date_str = f"{year}-{month:02d}"
    # ID=466ã¯è¥¿å®®å¸‚ã®ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ID
    return f"https://www.nishi.or.jp/homepage/gomicalendar/calendar_b.html?date={date_str}&id=466#garbage-calendar"

def get_weekday_str(year, month, day):
    try:
        dt = datetime.date(year, month, int(day))
        weekdays = ["æœˆ", "ç«", "æ°´", "æœ¨", "é‡‘", "åœŸ", "æ—¥"]
        return weekdays[dt.weekday()]
    except ValueError:
        return ""

@st.cache_data(ttl=3600)  # 1æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def fetch_calendar_data():
    """ä»Šæœˆã¨æ¥æœˆã®ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ã¾ã¨ã‚ã¦å–å¾—"""
    now = datetime.datetime.now()
    years_months = [(now.year, now.month)]
    
    # æ¥æœˆã®è¨ˆç®—
    if now.month == 12:
        years_months.append((now.year + 1, 1))
    else:
        years_months.append((now.year, now.month + 1))
    
    all_data = []
    
    for year, month in years_months:
        url = get_url_by_date(year, month)
        try:
            response = requests.get(url, timeout=10)
            response.encoding = response.apparent_encoding
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                calendar_table = soup.find('table')
                if calendar_table:
                    rows = calendar_table.find_all('tr')
                    for row in rows:
                        cols = row.find_all('td')
                        for col in cols:
                            text = col.get_text(strip=True)
                            if text:
                                match = re.match(r"(\d+)(.*)", text)
                                if match:
                                    day_num = int(match.group(1))
                                    gomi_type = match.group(2)
                                    date_obj = datetime.date(year, month, day_num)
                                    # éå»ãƒ‡ãƒ¼ã‚¿ã¯é™¤å¤–ï¼ˆä»Šæ—¥ä»¥é™ã®ã¿ï¼‰
                                    if date_obj >= now.date():
                                        all_data.append({
                                            "date_obj": date_obj,
                                            "æ—¥ä»˜": f"{month}/{day_num}",
                                            "æ›œæ—¥": get_weekday_str(year, month, day_num),
                                            "ã‚´ãƒŸã®ç¨®é¡": gomi_type
                                        })
        except Exception:
            pass
            
    # æ—¥ä»˜é †ã«ä¸¦ã¹æ›¿ãˆ
    df = pd.DataFrame(all_data)
    if not df.empty:
        df = df.sort_values('date_obj')
    return df

# ==========================================
# 2. åˆ†åˆ¥ã‚¬ã‚¤ãƒ‰è©³ç´°å–å¾—æ©Ÿèƒ½
# ==========================================
@st.cache_data(ttl=86400) # 1æ—¥ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def fetch_detailed_guide():
    base_url = "https://www.nishi.or.jp/kurashi/gomi/gominoshushu/gominobunnbetu.html"
    guide_data = []
    try:
        res = requests.get(base_url, timeout=10)
        res.encoding = res.apparent_encoding
        soup = BeautifulSoup(res.text, 'html.parser')
        
        content_area = soup.find('div', id='main') or soup.find('div', id='contents')
        if not content_area: return []

        links = content_area.find_all('a')
        target_urls = []
        for link in links:
            href = link.get('href')
            text = link.get_text(strip=True)
            if href and text:
                keywords = ["ã‚‚ã‚„ã™ã”ã¿", "ç‡ƒã‚„ã•ãªã„ã”ã¿", "è³‡æº", "ãƒšãƒƒãƒˆãƒœãƒˆãƒ«", "ãƒ—ãƒ©", "å±é™º"]
                if any(k in text for k in keywords):
                    full_url = urljoin(base_url, href)
                    target_urls.append((text, full_url))
        
        target_urls = list(set(target_urls))

        for title, link_url in target_urls:
            try:
                sub_res = requests.get(link_url, timeout=5)
                sub_res.encoding = sub_res.apparent_encoding
                sub_soup = BeautifulSoup(sub_res.text, 'html.parser')
                sub_content = sub_soup.find('div', id='main') or sub_soup.find('div', id='contents')
                if sub_content:
                    for script in sub_content(["script", "style"]):
                        script.decompose()
                    details_text = sub_content.get_text("\n", strip=True)
                    mapped_category = map_guide_to_calendar(title)
                    guide_data.append({
                        "category_name": title,
                        "calendar_name": mapped_category,
                        "details": details_text,
                        "url": link_url
                    })
            except Exception:
                continue
        return guide_data
    except Exception:
        return []

def map_guide_to_calendar(guide_title):
    mapping = {
        "ã‚‚ã‚„ã™ã”ã¿": "ç‡ƒã‚„ã™ã”ã¿", "ç‡ƒã‚„ã•ãªã„ã”ã¿": "ç‡ƒã‚„ã•ãªã„ã”ã¿",
        "è³‡æºA": "è³‡æºA", "è³‡æºB": "è³‡æºB",
        "ãã®ä»–ãƒ—ãƒ©": "ãã®ä»–ãƒ—ãƒ©", "ãƒšãƒƒãƒˆãƒœãƒˆãƒ«": "ãƒšãƒƒãƒˆãƒœãƒˆãƒ«",
    }
    for key, val in mapping.items():
        if key in guide_title: return val
    return guide_title

# ==========================================
# 3. ãƒ¡ã‚¤ãƒ³è¡¨ç¤ºå‡¦ç†
# ==========================================
def main():
    st.title("ğŸ—‘ï¸ è¥¿å®®å¸‚ ã”ã¿åé›†ãƒŠãƒ“")

    with st.spinner('ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ã—ã¦ã„ã¾ã™...'):
        df_calendar = fetch_calendar_data()
        guide_list = fetch_detailed_guide()

    tab1, tab2 = st.tabs(["ğŸ“… ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼", "ğŸ” åˆ†åˆ¥ãƒ»æ¤œç´¢"])

    # -----------------------
    # ã‚¿ãƒ–1: ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼
    # -----------------------
    with tab1:
        # å…¬å¼ã‚µã‚¤ãƒˆã¸ã®ãƒªãƒ³ã‚¯
        now = datetime.datetime.now()
        current_month_url = get_url_by_date(now.year, now.month)
        st.markdown(f"**å…¬å¼ã‚µã‚¤ãƒˆã§ç¢ºèª:** [ğŸ‘‰ è¥¿å®®å¸‚ã”ã¿ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ ({now.month}æœˆåˆ†)]({current_month_url})")

        if df_calendar is not None and not df_calendar.empty:
            today_date = now.date()
            
            # ä»Šæ—¥ä»¥é™ã®ãƒ‡ãƒ¼ã‚¿ã ã‘ã‚’ä½¿ã†
            future_df = df_calendar[df_calendar['date_obj'] >= today_date]

            if not future_df.empty:
                # === ä»Šæ—¥ã®åé›† ===
                today_df = future_df[future_df['date_obj'] == today_date]
                if not today_df.empty:
                    row = today_df.iloc[0]
                    st.markdown("### ğŸ“… ä»Šæ—¥ã®åé›†")
                    st.success(f"**ä»Šæ—¥ã¯ {row['æ—¥ä»˜']} ({row['æ›œæ—¥']})**")
                    st.markdown(f"<h1 style='text-align: center; color: #ff4b4b;'>{row['ã‚´ãƒŸã®ç¨®é¡']}</h1>", unsafe_allow_html=True)
                else:
                    next_row = future_df.iloc[0]
                    st.info(f"ä»Šæ—¥ã¯åé›†ãŒã‚ã‚Šã¾ã›ã‚“ã€‚æ¬¡ã¯ {next_row['æ—¥ä»˜']} ({next_row['æ›œæ—¥']}) ã® {next_row['ã‚´ãƒŸã®ç¨®é¡']} ã§ã™ã€‚")
                
                st.divider()
                
                # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼šå‘ã“ã†1é€±é–“ (7ä»¶) ã¨ ãã‚Œä»¥é™
                one_week_df = future_df.head(7)
                rest_df = future_df.iloc[7:]

                # === å‘ã“ã†1é€±é–“ã®äºˆå®š ===
                st.subheader("ğŸ“‹ å‘ã“ã†1é€±é–“ã®äºˆå®š")
                st.table(one_week_df[['æ—¥ä»˜', 'æ›œæ—¥', 'ã‚´ãƒŸã®ç¨®é¡']].set_index('æ—¥ä»˜'))

                # === â˜…å¾©æ´»: æ¬¡å›ä»¥é™ã®ã‚¤ãƒ¬ã‚®ãƒ¥ãƒ©ãƒ¼ã”ã¿ ===
                st.subheader("ğŸ‘€ æ¬¡å›ä»¥é™ã®äºˆå®š (1é€±é–“ä»¥å†…ã«ãªã„ã‚‚ã®)")
                
                types_in_week = set(one_week_df['ã‚´ãƒŸã®ç¨®é¡'].unique())
                types_in_rest = set(rest_df['ã‚´ãƒŸã®ç¨®é¡'].unique())
                
                # ã€Œæœªæ¥ã«ã¯ã‚ã‚‹ã€ã‘ã©ã€Œç›´è¿‘1é€±é–“ã«ã¯ãªã„ã€ã‚´ãƒŸ
                missing_types = types_in_rest - types_in_week
                
                if missing_types:
                    found_count = 0
                    for g_type in missing_types:
                        # ãã®ã‚´ãƒŸã®æœ€çŸ­ã®æ—¥ä»˜ã‚’æ¢ã™
                        next_match = rest_df[rest_df['ã‚´ãƒŸã®ç¨®é¡'] == g_type]
                        if not next_match.empty:
                            next_row = next_match.iloc[0]
                            st.info(f"**{g_type}** ã¯ã€å°‘ã—å…ˆã® **{next_row['æ—¥ä»˜']} ({next_row['æ›œæ—¥']})** ã§ã™")
                            found_count += 1
                    
                    if found_count == 0:
                         st.caption("â€»ã“ã‚Œä»¥å¤–ã®åé›†äºˆå®šã¯ä»Šã®ã¨ã“ã‚ã‚ã‚Šã¾ã›ã‚“ã€‚")
                else:
                    st.caption("â€»ä¸»è¦ãªã‚´ãƒŸã¯ã™ã¹ã¦1é€±é–“ä»¥å†…ã«åé›†ãŒã‚ã‚Šã¾ã™ã€‚")

            else:
                 st.warning("ã“ã‚Œä»¥é™ã®åé›†äºˆå®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

        else:
            st.error("ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            st.link_button("å…¬å¼ã‚µã‚¤ãƒˆã‚’ç›´æ¥è¦‹ã‚‹", current_month_url)

    # -----------------------
    # ã‚¿ãƒ–2: åˆ†åˆ¥ã‚¬ã‚¤ãƒ‰
    # -----------------------
    with tab2:
        st.header("ğŸ” ã”ã¿åˆ†åˆ¥æ¤œç´¢")
        query = st.text_input("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (ä¾‹: é›»æ± , ãƒ•ãƒ©ã‚¤ãƒ‘ãƒ³)", "")

        if query:
            found_count = 0
            for item in guide_list:
                if query in item['details'] or query in item['category_name']:
                    found_count += 1
                    cat_name = item['category_name']
                    cal_name = item['calendar_name']
                    
                    with st.container():
                        st.markdown(f"### ğŸ’¡ {cat_name} ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
                        
                        if df_calendar is not None and not df_calendar.empty:
                            # ä»Šæ—¥ä»¥é™ã®ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼ã‹ã‚‰æ¢ã™
                            matches = df_calendar[
                                (df_calendar['ã‚´ãƒŸã®ç¨®é¡'].str.contains(cal_name, na=False)) &
                                (df_calendar['date_obj'] >= datetime.datetime.now().date())
                            ]
                            if not matches.empty:
                                next_pickup = matches.iloc[0]
                                st.success(f"**æ¬¡ã®åé›†æ—¥:** ğŸ“… **{next_pickup['æ—¥ä»˜']} ({next_pickup['æ›œæ—¥']})**")
                        
                        with st.expander("è©³ã—ã„å‡ºã—æ–¹ã‚’è¦‹ã‚‹"):
                            st.markdown(f"[å…¬å¼ãƒšãƒ¼ã‚¸ã§è¦‹ã‚‹]({item['url']})")
                            preview = item['details'][:300] + "..." if len(item['details']) > 300 else item['details']
                            st.text(preview)
                        st.divider()
            if found_count == 0:
                st.warning(f"ã€Œ{query}ã€ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            st.info("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã™ã‚‹ã¨çµæœãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")
            with st.expander("ã‚«ãƒ†ã‚´ãƒªä¸€è¦§"):
                for item in guide_list:
                    st.write(f"- [{item['category_name']}]({item['url']})")

if __name__ == "__main__":
    main()